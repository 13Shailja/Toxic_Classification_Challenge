{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building a CNN model for the classification\n"
      ],
      "metadata": {
        "id": "Nk0VllyJxfVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assigning the processed_lemmatized_words and preprocessed_comment columns from the data DataFrame to the X_cnn and X_cnn_noLemma variables, respectively.\n",
        "# X_cnn = data.processed_lemmatized_words.values\n",
        "# X_cnn_noLemma = data.preprocessed_comment.values\n",
        "\n",
        "# # Assigning the toxic, severe_toxic, obscene, threat, insult, and identity_hate columns from the data DataFrame to the y_cnn DataFrame.\n",
        "# y_cnn = data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]"
      ],
      "metadata": {
        "id": "iLxuKLDlAK2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining constants\n",
        "MAX_SEQUENCE_LENGTH = 1000  # The maximum length of a sequence.\n",
        "MAX_NUM_WORDS = 20000  # The maximum vocabulary size.\n",
        "EMBEDDING_DIM = 100  # The embedding dimension.\n",
        "VALIDATION_SPLIT = 0.2  # The fraction of the data to use for validation."
      ],
      "metadata": {
        "id": "o1531vG___DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the class\n",
        "# This creates a tokenizer object with a maximum vocabulary size of MAX_NUM_WORDS.\n",
        "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "tokenizer_noLemma = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "\n",
        "# Updating internal vocabulary based on a list of texts.\n",
        "# This fits the tokenizer to the text data in X_cnn. The tokenizer will learn the vocabulary of the text data and create a mapping from words to integers.\n",
        "tokenizer.fit_on_texts(X)\n",
        "tokenizer_noLemma.fit_on_texts(X_noLemma)"
      ],
      "metadata": {
        "id": "EUndLSsn_vSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming each text in texts to a sequence of integers\n",
        "# This function takes a list of texts and returns a list of sequences of integers, where each sequence represents the words in a text. The integers represent the index of the word in the vocabulary.\n",
        "train_sequences = tokenizer.texts_to_sequences(X)\n",
        "train_sequences_noLemma = tokenizer_noLemma.texts_to_sequences(X_noLemma)"
      ],
      "metadata": {
        "id": "rs8IXNnQAfqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the word index from the tokenizer - The word index is a mapping from words to integers.\n",
        "word_index = tokenizer.word_index\n",
        "word_index_noLemma = tokenizer_noLemma.word_index\n",
        "\n",
        "# Printing the length of the word index - This shows how many words are in the vocabulary.\n",
        "print(\"Length of word Index:\", len(word_index))\n",
        "\n",
        "# Printing the first 5 elements in the word index dictionary\n",
        "# This shows the first 5 words in the vocabulary and their corresponding indices.\n",
        "print(\"First 5 elements in the word_index dictionary:\", dict(list(word_index.items())[0: 5]))\n",
        "\n",
        "# Printing the first comment text in the training set\n",
        "# This shows the first comment text in the training set, represented as a sequence of integers.\n",
        "print(\"First comment text in training set:\\n\", train_sequences[0])\n",
        "\n",
        "# With no lemmatized comment\n",
        "# Repeating the above steps for the training set without lemmatization.\n",
        "print(\"\\n\")\n",
        "print(\"Length of word Index:\", len(word_index_noLemma))\n",
        "print(\"First 5 elements in the word_index dictionary:\", dict(list(word_index_noLemma.items())[0: 5]))\n",
        "print(\"First comment text in training set:\\n\", train_sequences_noLemma[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxiTnM8VAfa3",
        "outputId": "a888921e-173b-43c3-9be4-08b223f97c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of word Index: 67330\n",
            "First 5 elements in the word_index dictionary: {'article': 1, 'page': 2, 'fuck': 3, 'wikipedia': 4, 'like': 5}\n",
            "First comment text in training set:\n",
            " [1898, 46, 37, 1101, 46, 8]\n",
            "\n",
            "\n",
            "Length of word Index: 72638\n",
            "First 5 elements in the word_index dictionary: {'fuck': 1, 'article': 2, 'page': 3, 'wikipedia': 4, 'like': 5}\n",
            "First comment text in training set:\n",
            " [1953, 39, 32, 2321, 39, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad tokenized sequences\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"Shape of padded sequence list:\\n\", train_data.shape)\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data[0][-50:])\n",
        "\n",
        "\n",
        "#Pad tokenized sequences\n",
        "train_data_noLemma = pad_sequences(train_sequences_noLemma, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"Shape of padded sequence list:\\n\", train_data_noLemma.shape)\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data_noLemma[0][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4NpSh5Qt_YX",
        "outputId": "7431b250-521e-4ed1-cfcb-10656641995d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0 1898   46   37 1101   46    8]\n",
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0 1953   39   32 2321   39    7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding tokenized sequences - This function pads the sequences to the maximum sequence length, which is defined by the MAX_SEQUENCE_LENGTH constant.\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Printing the shape of the padded sequence list - showing the shape of the padded sequence list, which is (number of samples, maximum sequence length).\n",
        "print(\"Shape of padded sequence list:\\n\", train_data.shape)\n",
        "\n",
        "# Printing the first comment text in the training set - showing the first comment text in the training set, represented as a sequence of integers. Only the last 50 sequences are printed, as the rest are paddings.\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data[0][-50:])\n",
        "\n",
        "# With no lemmatized comment\n",
        "# Repeating the above steps for the training set without lemmatization.\n",
        "print(\"\\n\")\n",
        "train_data_noLemma = pad_sequences(train_sequences_noLemma, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Shape of padded sequence list:\\n\", train_data_noLemma.shape)\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data_noLemma[0][-50:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-SidhpDBA8M",
        "outputId": "7c2ce006-4eaf-447e-bd6d-2b2cb964d436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0 1898   46   37 1101   46    8]\n",
            "\n",
            "\n",
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0 1953   39   32 2321   39    7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Sequential model - which is a linear stack of layers.\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Adding an Embedding layer\n",
        "# This layer creates an embedding matrix, which maps each word in the vocabulary to a vector of 128 dimensions.\n",
        "cnn_model.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "\n",
        "# Adding a Conv1D layer\n",
        "# This layer performs 1D convolutions on the embedding layer output. The kernel size is 5, and the activation function is ReLU.\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
        "\n",
        "# Adding a MaxPooling1D layer\n",
        "# This layer performs max pooling on the output of the Conv1D layer. The pool size is 5.\n",
        "cnn_model.add(MaxPooling1D(pool_size=5))\n",
        "\n",
        "# Adding another Conv1D and MaxPooling1D layer\n",
        "# Repeat the above steps to add another Conv1D and MaxPooling1D layer.\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
        "cnn_model.add(MaxPooling1D(pool_size=5))\n",
        "\n",
        "# Adding a GlobalMaxPooling1D layer\n",
        "# This layer performs global max pooling on the output of the last Conv1D layer. This reduces the output shape to (batch_size, 128).\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# Adding a Dense layer\n",
        "# This layer performs a dense layer with 128 units and ReLU activation function.\n",
        "cnn_model.add(Dense(units=128, activation='relu'))\n",
        "\n",
        "# Adding a final Dense layer\n",
        "# This layer performs a dense layer with 6 units and sigmoid activation function. This is the output layer, which predicts the toxicity labels.\n",
        "cnn_model.add(Dense(units=6, activation='sigmoid'))\n",
        "\n",
        "# Printing the model summary\n",
        "# This prints a summary of the model, including the layer sizes and output shapes.\n",
        "print(cnn_model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv2elhWtBWt8",
        "outputId": "b0667e52-9bcb-431c-cf39-f6d440ede466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,741,382\n",
            "Trainable params: 2,741,382\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the model for training\n",
        "# This sets the loss function, optimizer, and metrics for the model.\n",
        "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\n",
        "\n",
        "# Splitting the dataset into train and validation set\n",
        "# This splits the dataset into two sets, a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate the model's performance.\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, y, shuffle=True, random_state=123)\n",
        "\n",
        "# Printing the shape of the train and validation sets\n",
        "# This prints the shape of the train and validation sets, which is (number of samples, maximum sequence length).\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "# Training the model for a fixed number of epochs\n",
        "# This trains the model for 1 epoch, which is one pass through the entire training set. The validation data is used to evaluate the model's performance after each epoch.\n",
        "history = cnn_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RogadMuOBxC4",
        "outputId": "73c0b569-4e37-4c57-db0b-b88d53da103c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24337, 1000) (24337, 6) (8113, 1000) (8113, 6)\n",
            "Epoch 1/10\n",
            "191/191 [==============================] - 41s 204ms/step - loss: 0.2935 - auc: 0.8926 - val_loss: 0.2216 - val_auc: 0.9415\n",
            "Epoch 2/10\n",
            "191/191 [==============================] - 31s 161ms/step - loss: 0.1995 - auc: 0.9535 - val_loss: 0.2180 - val_auc: 0.9434\n",
            "Epoch 3/10\n",
            "191/191 [==============================] - 22s 115ms/step - loss: 0.1750 - auc: 0.9653 - val_loss: 0.2330 - val_auc: 0.9375\n",
            "Epoch 4/10\n",
            "191/191 [==============================] - 19s 98ms/step - loss: 0.1516 - auc: 0.9745 - val_loss: 0.2590 - val_auc: 0.9272\n",
            "Epoch 5/10\n",
            "191/191 [==============================] - 18s 92ms/step - loss: 0.1299 - auc: 0.9814 - val_loss: 0.2757 - val_auc: 0.9230\n",
            "Epoch 6/10\n",
            "191/191 [==============================] - 14s 71ms/step - loss: 0.1100 - auc: 0.9866 - val_loss: 0.2982 - val_auc: 0.9198\n",
            "Epoch 7/10\n",
            "191/191 [==============================] - 12s 62ms/step - loss: 0.0926 - auc: 0.9905 - val_loss: 0.3568 - val_auc: 0.9100\n",
            "Epoch 8/10\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.0770 - auc: 0.9933 - val_loss: 0.3973 - val_auc: 0.8969\n",
            "Epoch 9/10\n",
            "191/191 [==============================] - 12s 61ms/step - loss: 0.0649 - auc: 0.9952 - val_loss: 0.4549 - val_auc: 0.8926\n",
            "Epoch 10/10\n",
            "191/191 [==============================] - 11s 59ms/step - loss: 0.0563 - auc: 0.9962 - val_loss: 0.5141 - val_auc: 0.8817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the CNN model"
      ],
      "metadata": {
        "id": "Jqr_NdrR0h2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the input string to a sequence of integers\n",
        "test_sequences = tokenizer.texts_to_sequences('I will kill you')\n",
        "\n",
        "# Padding the sequence to the maximum sequence length\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Using the CNN model to predict the toxicity levels of the input string\n",
        "y_preds = cnn_model.predict(test_data)\n",
        "\n",
        "# Printing the toxicity levels of the input string\n",
        "print('Toxic:         {:.0%}'.format(y_preds[0][0]))\n",
        "print('Severe Toxic:  {:.0%}'.format(y_preds[0][1]))\n",
        "print('Obscene:       {:.0%}'.format(y_preds[0][2]))\n",
        "print('Threat:        {:.0%}'.format(y_preds[0][3]))\n",
        "print('Insult:        {:.0%}'.format(y_preds[0][4]))\n",
        "print('Identity Hate: {:.0%}'.format(y_preds[0][5]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69FlQNHWCjB1",
        "outputId": "3ef0134d-2d6b-47fc-d693-85e62263cbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 227ms/step\n",
            "Toxic:         35%\n",
            "Severe Toxic:  3%\n",
            "Obscene:       17%\n",
            "Threat:        4%\n",
            "Insult:        19%\n",
            "Identity Hate: 5%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, the above CNN model cannot detect Threat in the text message"
      ],
      "metadata": {
        "id": "ze5_4iXQFENx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "# def toxicity_level(string, model):\n",
        "#     \"\"\"\n",
        "#     Return toxicity probability based on inputed string.\n",
        "\n",
        "#     Args:\n",
        "#         string (str): The string to be analyzed.\n",
        "#         model (keras.Model): The trained model.\n",
        "\n",
        "#     Returns:\n",
        "#         The toxicity levels of the input string.\n",
        "#     \"\"\"\n",
        "#     # Process string\n",
        "#     # This code takes the input string and converts it into a sequence of integers. The sequence is then padded to the maximum sequence length.\n",
        "#     new_string = [string]\n",
        "#     new_string = tokenizer.texts_to_sequences(new_string)\n",
        "#     new_string = pad_sequences(new_string, maxlen=maxlen, padding='pre')\n",
        "\n",
        "#     # Predict\n",
        "#     # This code uses the model to predict the toxicity levels of the input string.\n",
        "#     prediction = model.predict(new_string)\n",
        "\n",
        "#     # Print output\n",
        "#     # This code prints the toxicity levels of the input string.\n",
        "#     print(\"Toxicity levels for '{}':\".format(string))\n",
        "#     print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
        "#     print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
        "#     print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
        "#     print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
        "#     print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
        "#     print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
        "#     print()\n",
        "\n",
        "#     return prediction"
      ],
      "metadata": {
        "id": "oVS4ISiO2ZPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a CNN model for the classification - 3 epochs\n",
        "\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are a type of deep learning neural network that are commonly used for image recognition and processing.\n",
        "CNNs are inspired by the way the human visual cortex works, and they are able to learn to recognize patterns in images by applying a series of convolution operations.\n",
        "\n",
        "\n",
        "Convolution is a mathematical operation that takes two functions as input and produces a third function that expresses how the shape of one function is modified by the other function. In the context of CNNs, the two functions are the image and a filter. The filter is a small matrix of weights that is used to scan the image, and the convolution operation produces a new image that highlights the features that are detected by the filter.\n",
        "\n",
        "\n",
        "CNNs typically have three types of layers: convolutional layers, pooling layers, and fully-connected layers. The convolutional layers are responsible for detecting features in the image, the pooling layers are responsible for reducing the size of the image while preserving the most important features, and the fully-connected layers are responsible for classifying the image.\n",
        "\n",
        "\n",
        "Here are some of the benefits of using CNNs:\n",
        "\n",
        "* They are able to learn to recognize patterns in images without being explicitly programmed to do so.\n",
        "* They are able to generalize to new images that they have not seen before.\n",
        "* They are able to process images very quickly.\n",
        "\n",
        "However, CNNs also have some limitations:\n",
        "\n",
        "* They require a large amount of training data.\n",
        "* They can be computationally expensive to train.\n",
        "* They can be difficult to interpret."
      ],
      "metadata": {
        "id": "xZS0Vl8XbbC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the processed_lemmatized_words and preprocessed_comment columns from the data DataFrame to the X_cnn and X_cnn_noLemma variables, respectively.\n",
        "X_cnn = data.processed_lemmatized_words.values\n",
        "X_cnn_noLemma = data.preprocessed_comment.values\n",
        "\n",
        "# Assigning the toxic, severe_toxic, obscene, threat, insult, and identity_hate columns from the data DataFrame to the y_cnn DataFrame.\n",
        "y_cnn = data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"
      ],
      "metadata": {
        "id": "Zegs1Fo_bbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining constants\n",
        "MAX_SEQUENCE_LENGTH = 1000  # The maximum length of a sequence.\n",
        "MAX_NUM_WORDS = 20000  # The maximum vocabulary size.\n",
        "EMBEDDING_DIM = 100  # The embedding dimension.\n",
        "VALIDATION_SPLIT = 0.2  # The fraction of the data to use for validation."
      ],
      "metadata": {
        "id": "ne5_TzKRbbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the class\n",
        "# This creates a tokenizer object with a maximum vocabulary size of MAX_NUM_WORDS.\n",
        "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "tokenizer_noLemma = Tokenizer(num_words = MAX_NUM_WORDS)\n",
        "\n",
        "# Updating internal vocabulary based on a list of texts.\n",
        "# This fits the tokenizer to the text data in X_cnn. The tokenizer will learn the vocabulary of the text data and create a mapping from words to integers.\n",
        "tokenizer.fit_on_texts(X)\n",
        "tokenizer_noLemma.fit_on_texts(X_noLemma)"
      ],
      "metadata": {
        "id": "ioOIQgq6bbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming each text in texts to a sequence of integers\n",
        "# This function takes a list of texts and returns a list of sequences of integers, where each sequence represents the words in a text. The integers represent the index of the word in the vocabulary.\n",
        "train_sequences = tokenizer.texts_to_sequences(X)\n",
        "train_sequences_noLemma = tokenizer_noLemma.texts_to_sequences(X_noLemma)"
      ],
      "metadata": {
        "id": "LhdMGKLxbbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the word index from the tokenizer - The word index is a mapping from words to integers.\n",
        "word_index = tokenizer.word_index\n",
        "word_index_noLemma = tokenizer_noLemma.word_index\n",
        "\n",
        "# Printing the length of the word index - This shows how many words are in the vocabulary.\n",
        "print(\"Length of word Index:\", len(word_index))\n",
        "\n",
        "# Printing the first 5 elements in the word index dictionary\n",
        "# This shows the first 5 words in the vocabulary and their corresponding indices.\n",
        "print(\"First 5 elements in the word_index dictionary:\", dict(list(word_index.items())[0: 5]))\n",
        "\n",
        "# Printing the first comment text in the training set\n",
        "# This shows the first comment text in the training set, represented as a sequence of integers.\n",
        "print(\"First comment text in training set:\\n\", train_sequences[0])\n",
        "\n",
        "# With no lemmatized comment\n",
        "# Repeating the above steps for the training set without lemmatization.\n",
        "print(\"\\n\")\n",
        "print(\"Length of word Index:\", len(word_index_noLemma))\n",
        "print(\"First 5 elements in the word_index dictionary:\", dict(list(word_index_noLemma.items())[0: 5]))\n",
        "print(\"First comment text in training set:\\n\", train_sequences_noLemma[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dd1ae2-9e4e-4927-ed06-bb583a1618da",
        "id": "GNTWiIAMbbDD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of word Index: 67669\n",
            "First 5 elements in the word_index dictionary: {'article': 1, 'page': 2, 'fuck': 3, 'wikipedia': 4, 'like': 5}\n",
            "First comment text in training set:\n",
            " [296, 888, 205, 67]\n",
            "\n",
            "\n",
            "Length of word Index: 72886\n",
            "First 5 elements in the word_index dictionary: {'fuck': 1, 'article': 2, 'page': 3, 'wikipedia': 4, 'like': 5}\n",
            "First comment text in training set:\n",
            " [297, 956, 190, 76]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding tokenized sequences - This function pads the sequences to the maximum sequence length, which is defined by the MAX_SEQUENCE_LENGTH constant.\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Printing the shape of the padded sequence list - showing the shape of the padded sequence list, which is (number of samples, maximum sequence length).\n",
        "print(\"Shape of padded sequence list:\\n\", train_data.shape)\n",
        "\n",
        "# Printing the first comment text in the training set - showing the first comment text in the training set, represented as a sequence of integers. Only the last 50 sequences are printed, as the rest are paddings.\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data[0][-50:])\n",
        "\n",
        "# With no lemmatized comment\n",
        "# Repeating the above steps for the training set without lemmatization.\n",
        "print(\"\\n\")\n",
        "train_data_noLemma = pad_sequences(train_sequences_noLemma, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Shape of padded sequence list:\\n\", train_data_noLemma.shape)\n",
        "print(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", train_data_noLemma[0][-50:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc913d9-3281-4665-d528-15cd86964202",
        "id": "GDh8i5vVbbDE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 296 888 205  67]\n",
            "\n",
            "\n",
            "Shape of padded sequence list:\n",
            " (32450, 1000)\n",
            "First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 297 956 190  76]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Sequential model - which is a linear stack of layers.\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Adding an Embedding layer\n",
        "# This layer creates an embedding matrix, which maps each word in the vocabulary to a vector of 128 dimensions.\n",
        "cnn_model.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "\n",
        "# Adding a Conv1D layer\n",
        "# This layer performs 1D convolutions on the embedding layer output. The kernel size is 5, and the activation function is ReLU.\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
        "\n",
        "# Adding a MaxPooling1D layer\n",
        "# This layer performs max pooling on the output of the Conv1D layer. The pool size is 5.\n",
        "cnn_model.add(MaxPooling1D(pool_size=5))\n",
        "\n",
        "# Adding another Conv1D and MaxPooling1D layer\n",
        "# Repeat the above steps to add another Conv1D and MaxPooling1D layer.\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
        "cnn_model.add(MaxPooling1D(pool_size=5))\n",
        "\n",
        "# Adding a GlobalMaxPooling1D layer\n",
        "# This layer performs global max pooling on the output of the last Conv1D layer. This reduces the output shape to (batch_size, 128).\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# Adding a Dense layer\n",
        "# This layer performs a dense layer with 128 units and ReLU activation function.\n",
        "cnn_model.add(Dense(units=128, activation='relu'))\n",
        "\n",
        "# Adding a final Dense layer\n",
        "# This layer performs a dense layer with 6 units and sigmoid activation function. This is the output layer, which predicts the toxicity labels.\n",
        "cnn_model.add(Dense(units=6, activation='sigmoid'))\n",
        "\n",
        "# Printing the model summary\n",
        "# This prints a summary of the model, including the layer sizes and output shapes.\n",
        "print(cnn_model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d233fe-e9fc-4030-ba7f-afb396b43ccd",
        "id": "3TiNXFQgbbDE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,741,382\n",
            "Trainable params: 2,741,382\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the model for training\n",
        "# This sets the loss function, optimizer, and metrics for the model.\n",
        "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\"])\n",
        "\n",
        "# Splitting the dataset into train and validation set\n",
        "# This splits the dataset into two sets, a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate the model's performance.\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, y, shuffle=True, random_state=123)\n",
        "\n",
        "# Printing the shape of the train and validation sets\n",
        "# This prints the shape of the train and validation sets, which is (number of samples, maximum sequence length).\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "# Training the model for a fixed number of epochs\n",
        "# This trains the model for 1 epoch, which is one pass through the entire training set. The validation data is used to evaluate the model's performance after each epoch.\n",
        "history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_val, y_val), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654aee21-66a4-4752-e05f-b8a766b511ab",
        "id": "yKrABXpYbbDE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24337, 1000) (24337, 6) (8113, 1000) (8113, 6)\n",
            "Epoch 1/3\n",
            "191/191 [==============================] - 46s 202ms/step - loss: 0.3083 - auc: 0.8789 - val_loss: 0.2262 - val_auc: 0.9389\n",
            "Epoch 2/3\n",
            "191/191 [==============================] - 31s 162ms/step - loss: 0.2017 - auc: 0.9526 - val_loss: 0.2230 - val_auc: 0.9407\n",
            "Epoch 3/3\n",
            "191/191 [==============================] - 21s 112ms/step - loss: 0.1753 - auc: 0.9650 - val_loss: 0.2320 - val_auc: 0.9377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the CNN model"
      ],
      "metadata": {
        "id": "y0t30TdjbbDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the input string to a sequence of integers\n",
        "test_sequences = tokenizer.texts_to_sequences('I will kill you')\n",
        "\n",
        "# Padding the sequence to the maximum sequence length\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Using the CNN model to predict the toxicity levels of the input string\n",
        "y_preds = cnn_model.predict(test_data)\n",
        "\n",
        "# Printing the toxicity levels of the input string\n",
        "print('Toxic:         {:.0%}'.format(y_preds[0][0]))\n",
        "print('Severe Toxic:  {:.0%}'.format(y_preds[0][1]))\n",
        "print('Obscene:       {:.0%}'.format(y_preds[0][2]))\n",
        "print('Threat:        {:.0%}'.format(y_preds[0][3]))\n",
        "print('Insult:        {:.0%}'.format(y_preds[0][4]))\n",
        "print('Identity Hate: {:.0%}'.format(y_preds[0][5]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c59ab4-3dac-46e0-c0c5-ac024cc398c5",
        "id": "siGzX939bbDE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 135ms/step\n",
            "Toxic:         52%\n",
            "Severe Toxic:  4%\n",
            "Obscene:       23%\n",
            "Threat:        6%\n",
            "Insult:        26%\n",
            "Identity Hate: 8%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, the above CNN model cannot detect Threat in the text message"
      ],
      "metadata": {
        "id": "0lHJI3kJbbDE"
      }
    }
  ]
}